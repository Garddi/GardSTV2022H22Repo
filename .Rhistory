curve(a_map + b_map*(x), add = TRUE)
curve(a2_map + b2_map(x - xbar), add = TRUE)
b2_map <- mean(post2$b)
curve(a2_map + b2_map(x - xbar), add = TRUE)
curve(a2_map + b2_map*(x - xbar), add = TRUE)
curve(a_map + b_map*(x), add = TRUE)
#4M8
data("cherry_blossoms")
d <- cherry_blossoms
precis(d)
d2 <- d[complete.cases(d$doy),]
num_knots <- 25
knot_list <- quantile(d2$year, probs=seq(0,1, length.out = num_knots))
library(splines)
B <- bs(d2$year,
knots = knot_list[-c(1,num_knots)],
degree = 3, intercept = TRUE)
plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab="year", ylab="basis")
for (i in 1:ncol(B)) lines(d2$year, B[,1]
## R code 4.75
plot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab="year" , ylab="basis" )
## R code 4.75
plot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab="year" , ylab="basis" )
for ( i in 1:ncol(B) ) lines( d2$year , B[,i] )
## Knot number and prior weights control
## R code 4.76
m4.7 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + B %*% w ,
a ~ dnorm(100,10),
w ~ dnorm(0,10),
sigma ~ dexp(1)
), data=list( D=d2$doy , B=B ) ,
start=list( w=rep( 0 , ncol(B) ) ) )
## R code 4.77
post <- extract.samples( m4.7 )
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-6,6) ,
xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )
## R code 4.76
m4.7 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + B %*% w ,
a ~ dnorm(100,10),
w ~ dnorm(0,20),
sigma ~ dexp(1)
), data=list( D=d2$doy , B=B ) ,
start=list( w=rep( 0 , ncol(B) ) ) )
## R code 4.77
post <- extract.samples( m4.7 )
w <- apply( post$w , 2 , mean )
plot( NULL , xlim=range(d2$year) , ylim=c(-6,6) ,
xlab="year" , ylab="basis * weight" )
for ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )
p <- c(0.3, 0.7)
-sum(p*log(p))
p2 <- c(0.20, 0.25, 0.25, 0.30)
-sum(p2*log(p2))
p3 <- c(1/3, 1/3, 1/3)
-sum(p3*log(p3))
WAIC(m4.3)
m4.3a <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 10 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
m4.3a <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 10 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ]
# define the average weight, x-bar
xbar <- mean(d2$weight)
m4.3 <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*(weight) ,
a ~ dnorm( 178 , 20 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
m4.3a <- quap(
alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 10 ) ,
b ~ dlnorm( 0 , 1 ) ,
sigma ~ dunif( 0 , 50 )
) , data=d2 )
WAIC(m4.3)
WAIC(m4.3a)
d <- data("Laffer")
data("Laffer")
d <- Laffer
View(d)
lm1 <- (tax_revenue ~ tax_rate^2, data = d)
lm1 <- lm(tax_revenue ~ tax_rate^2, data = d)
ggplot(data = d, aes(x = tax_rate, y = tax_revenue)) +
geom_point(size = 2) + geom_smooth(method = "lm")
ggplot(data = d, aes(x = tax_rate, y = tax_revenue)) +
geom_point(size = 2) + geom_smooth(lm1)
ggplot(data = d, aes(x = tax_rate, y = tax_revenue)) +
geom_point(size = 2) + geom_smooth(method = NULL, formula = lm1)
ggplot(data = d, aes(x = tax_rate, y = tax_revenue)) +
geom_point(size = 2) + geom_smooth(method = NULL, formula = y ~ x^2)
View(Laffer)
#9M1
# Estimate the ruggedness model with a uniform prior
rm(list =ls())
data("rugged")
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[ complete.cases(d$rgdppc_2000) , ]
dd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)
dd$rugged_std <- dd$rugged / max(dd$rugged)
dd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )
## R code 9.13
dat_slim <- list(
log_gdp_std = dd$log_gdp_std,
rugged_std = dd$rugged_std,
cid = as.integer( dd$cid )
)
str(dat_slim)
## R code 9.14
m9.1 <- ulam(
alist(
log_gdp_std ~ dnorm( mu , sigma ) ,
mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
a[cid] ~ dnorm( 1 , 0.1 ) ,
b[cid] ~ dnorm( 0 , 0.3 ) ,
sigma ~ dexp( 1 )
) , data=dat_slim , chains=1 )
## R code 9.14
m9.1 <- ulam(
alist(
log_gdp_std ~ dnorm( mu , sigma ) ,
mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
a[cid] ~ dnorm( 1 , 0.1 ) ,
b[cid] ~ dnorm( 0 , 0.3 ) ,
sigma ~ dexp( 1 )
) , data=dat_slim , chains=1 )
rm(list = ls())
JapanSet <- readRDS("MA thesis/Reed-Smith-JHRED-CANDIDATES.rda")
JapanSet <- readRDS("MA thesis/Reed-Smith-JHRED-CANDIDATES")
JapanSet <- readRDS("MA thesis/Reed-Smith-JHRED-CANDIDATES.rds")
JapanSet <- readRDS("MA thesis/Reed-Smith-JHRED-CANDIDATES.Rdata")
load("MA thesis/Reed-Smith-JHRED-CANDIDATES.Rdata")
View(x)
x$name_jp[1]
JPMMM <- x %>%
filter(year = "1994" | "1995")
library(tidyverse)
JPMMM <- x %>%
filter(year = "1994" | "1995")
JPMMM <- x %>%
filter(year == "1994" | "1995")
JPMMM <- x %>%
filter(year == 1994 | 1995)
JPMMM <- x %>%
filter(year == 1994)
JPMMM <- x %>%
filter(year == 1993)
View(JPMMM)
JPMMM <- x %>%
filter(year >= 1993)
View(JPMMM)
table(JPMMM$ken)
JPMMM <- x %>%
filter(year > 1993)
View(JPMMM)
View(JPMMM)
?lm
lag(JPMMM, k=1, yr, name="previous")
lag(JPMMM, k=1, JPMMM$yr, name="previous")
JPMMM96 <- JPMMM %>%
filter(yr == 19.0)
JPMMM00 <- JPMMM %>%
filter(yr == 20.0)
JPMMM02 <- JPMMM %>%
filter(yr == 21.0)
JPMMM05 <- JPMMM %>%
filter(yr == 22.0)
JPMMM09 <- JPMMM %>%
filter(yr == 23.0)
JPMMM12 <- JPMMM %>%
filter(yr == 24.0)
JPMMM14 <- JPMMM %>%
filter(yr == 25.0)
mean(JPMMM[which(ken = "NA")]$female)
mean(JPMMM[which(JPMMM$ken = "NA")]$female)
mean(JPMMM[which(JPMMM$ken == "NA")]$female)
mean(JPMMM[which(ken == "NA")]$female)
mean(JPMMM[(which(ken == "NA"))]$female)
mean(JPMMM96[which(ken == "NA")]$female)
mean(JPMMM96[which(JPMMM96$ken == "NA")]$female)
which(JPMMM96$ken == "NA")
mean(JPMMM96$female[which(JPMMM96$ken == "NA")])
View(JPMMM96)
JPMMM96$female[364,]
JPMMM96$female[, 364]
JPMMM96$female[364]
JPMMM96$female[23]
JPMMM96$female[24]
JPMMM96$female[22]
?$
$?
mean(JPMMM96$female[which(JPMMM96$ken == "NA")])
mean(JPMMM00$female[which(JPMMM00$ken == "NA")])
mean(JPMMM02$female[which(JPMMM02$ken == "NA")])
mean(JPMMM05$female[which(JPMMM05$ken == "NA")])
mean(JPMMM09$female[which(JPMMM09$ken == "NA")])
mean(JPMMM12$female[which(JPMMM12$ken == "NA")])
mean(JPMMM14$female[which(JPMMM14$ken == "NA")])
mean(JPMMM96$female[which(JPMMM96$prcode > 0)])
mean(JPMMM00$female[which(JPMMM00$prcode > 0)])
mean(JPMMM02$female[which(JPMMM02$prcode > 0)])
mean(JPMMM05$female[which(JPMMM05$prcode > 0)])
mean(JPMMM09$female[which(JPMMM09$prcode > 0)])
mean(JPMMM12$female[which(JPMMM12$prcode > 0)])
mean(JPMMM14$female[which(JPMMM14$prcode > 0)])
load("C:/Users/gardi/OneDrive/Documents/Research for Japanese Politics/clea_lc_20201216.rdata")
View(x)
View(clea_lc_20201216)
rm(list = ls())
SKLegData <-
readRDS("MA thesis/SouthKorea Legislative Data/20160725_1320Ranked.dta")
SKLegData <-
read.csv("MA thesis/SouthKorea Legislative Data/20160725_1320Ranked.dta")
load("MA thesis/SouthKorea Legislative Data/20160725_1320Ranked.dta")
library(haven)
X20160725_1320Ranked <- read_dta("MA thesis/SouthKorea Legislative Data/20160725_1320Ranked.dta")
View(X20160725_1320Ranked)
X20160725_1320Ranked$sex[1]
load("MA thesis/Reed-Smith-JHRED-CANDIDATES.Rdata")
View(x)
View(rugged)
View(dd)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
#mean() gir oss gjennomsnittet av et objekt, brukes som regel på enkelt variabler
#spesifisert med dollartegnet
mean(Dataeksempel$kjonn)
#mean() gir oss gjennomsnittet av et objekt, brukes som regel på enkelt variabler
#spesifisert med dollartegnet
mean(Dataeksempel$kjonn)
?tibble
library(tidyverse)
install.packages("plotly")
install.packages("tidyverse")
install.packages("rlang")
install.packages("rlang")
install.packages("readxl")
install.packages("tidyverse")
install.packages("rlang")
install.packages("rlang")
install.packages("tidyverse")
library(tidyverse)
library(plotly)
remove.packages(c("StanHeaders", "rstan"))
Sys.getenv("BINPREF")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
library(plotly)
library(tidyverse)
View(d)
ggplot(d, aes(x = rugged, y = desert)) +
geom_point()
library(plotly)
library(tidyverse)
ggplot(d, aes(x = rugged, y = desert)) +
geom_point()
Sys.which("make")
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars.win")
if (!file.exists(M)) file.create(M)
cat("\n CXX14FLAGS += -mtune=native -O3 -mmmx -msse -msse2 -msse3 -mssse3 -msse4.1 -msse4.2",
file = M, sep = "\n", append = FALSE)
# only necessary for Linux without the nodejs library / headers
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
install.packages(c("coda", "mvtnorm", "devtools", "loo", "dagitty"))
devtools::install_github("rmcelreath/rethinking")
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
install.packages(c("coda", "mvtnorm", "devtools", "loo", "dagitty"))
install.packages(c("coda", "mvtnorm", "devtools", "loo", "dagitty"))
install.packages(c("coda", "mvtnorm", "devtools", "loo", "dagitty"))
install.packages("rlang")
install.packages("rlang")
update.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
View(d)
ggplot(d, aes(x = desert, y = rugged)) +
geom_point()
library(tidyverse)
install.packages("extrafont")
library(extrafont)
font_import(pattern = "lmroman*")
font_import(pattern = "lmroman*")
warnings()
rm(lst=ls())
rm(list=ls())
setwd("~/GardSTV2022H22Repo")
## Pakker ##
library(tidyverse)
regndans <- readLines("./data/regndans.txt")
bow <- regndans %>%
str_split("\\s") %>%
unlist()
set.seed(984301)
cat(bow[sample(1:length(bow))])
?cat
?bow
regndans[which(str_detect(regndans, "dragepust"))]
library(janeaustenr)
library(dplyr)
library(tidytext)
library(ggplot2)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(line = row_number()) %>%
ungroup()
View(original_books)
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
count(word) %>%
arrange(desc(n))
tidy_books %>% head(300) %>%
ggplot(., aes(x = 1:300, y = n)) +
geom_point() +
geom_line(aes(group = 1)) +
scale_y_continuous(trans = "log") +
scale_x_continuous(trans = "log") +
geom_smooth(method = "lm", se = FALSE) +
ggrepel::geom_label_repel(aes(label = word)) +
ggdark::dark_theme_classic() +
labs(x = "Rangering (log)", y = "Frekvens (log)", title = "Zipf's lov illustrasjon")
View(tidy_books)
library(tidytext)
load("./data/no4.rda")
no4_tokens <- no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst) %>%
count(token)
View(no4)
# Med stoppord
no4_tokens %>%
slice_max(order_by = n,
n = 2,
with_ties = FALSE)
# Uten stoppord
no4_tokens %>%
filter(token %in% quanteda::stopwords("no") == FALSE) %>%
slice_max(order_by = n,
n = 2,
with_ties = FALSE)
idf_stop <- no4_tokens %>%
bind_tf_idf(token, titler, n) %>%
ungroup() %>%
select(token, idf) %>%
unique() %>%
arrange(idf)
View(idf_stop)
idf_stop
idf_stop <- idf_stop %>%
filter(idf < 1)
View(idf_stop)
no4_tokens %>%
filter(token %in% idf_stop$token == FALSE) %>%
slice_max(order_by = n,
n = 2,
with_ties = FALSE)
no4_tokens <- no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst)
table(str_detect(no4_tokens$token, "[[:punct:]]"))
no4_tokens$token %>%
.[which(str_detect(., "[[:punct:]]"))]
no4_tokens <- no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
strip_punct = FALSE)
table(str_detect(no4_tokens$token, "[[:punct:]]"))
no4_tokens <- no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
strip_numeric = TRUE)
table(str_detect(no4_tokens$token, "[0-9]"))
stem1 <- tokenizers::tokenize_words("det satt to katter på et bord") %>%
unlist() %>%
quanteda::char_wordstem(., language = "no")
Sys.getlocale()
Sys.setlocale("LC_ALL", locale = "")
stem1 <- tokenizers::tokenize_words("det satt to katter på et bord") %>%
unlist() %>%
quanteda::char_wordstem(., language = "no")
stem2 <- tokenizers::tokenize_words("det satt en katt på et bordet") %>%
unlist() %>%
quanteda::char_wordstem(., language = "no")
cbind(stem1, stem2, samme = stem1 == stem2)
stem1 <- tokenizers::tokenize_words("jeg har én god fot og én dårlig hånd") %>%
unlist() %>%
quanteda::char_wordstem(., language = "no")
stem2 <- tokenizers::tokenize_words("jeg har to gode føtter og to dårlige hender") %>%
unlist() %>%
quanteda::char_wordstem(., language = "no")
tekst2 <- stortingscrape::read_obt("./data/lemmatisering/tekst2_tag.txt")
cbind(stem1, stem2, samme = stem1 == stem2)
library(spacyr)
install.packages("spacyr")
library(spacyr)
spacy_eksempel <- spacy_parse(c("jeg har én god fot og én dårlig hånd",
# "jeg har to gode føtter og to dårlige hender"))
spacy_eksempel <- spacy_parse(c("jeg har én god fot og én dårlig hånd",
"jeg har to gode føtter og to dårlige hender"))
# Må ha installert Python for å bruke spacyr
# library(spacyr)
spacy_initialize("nb_core_news_lg")
spacy_initialize("nb_core_news_lg")
library(spacyr)
spacy_initialize("nb_core_news_lg")
tekst2 <- stortingscrape::read_obt("./data/lemmatisering/tekst2_tag.txt")
library(stortingscrape)
tekst2 <- stortingscrape::read_obt("./data/lemmatisering/tekst2_tag.txt")
tekst2 <- stortingscrape::read_obt("./data/lemmatisering/tekst2_tag.txt")
tekst2 <- stortingscrape::read_obt("./data/tekst2_tag.txt")
tekst2
## n-grams ##
no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
token = "words")
## Pakker ##
library(tidyverse)
## n-grams ##
no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
token = "words")
library(janeaustenr)
library(dplyr)
library(tidytext)
library(ggplot2)
## n-grams ##
no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
token = "words")
no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
token = "ngrams",
n = 2)
Sys.setlocale("LC_ALL", "")
no4 %>%
group_by(spor, titler) %>%
unnest_tokens(output = token,
input = tekst,
token = "ngrams",
n = 2)
## Word embeddings ##
stoppord <- stopwords::stopwords("Norwegian") # Finner stoppord fra den norske bokmålslista til "stopwords" pakken
stoppord_boundary <- str_c("\\b", stoppord, "\\b", # Lager en vektor med "word boundary" for å ta ut ord fra en streng
collapse = "|") # Setter | mellom hver ord for å skille dem fra hverandre med "eller"-operator
no4_prepped <- no4 %>%
mutate(tekst = str_to_lower(tekst), # Setter all tekst til liten bokstav
tekst = str_replace_all(tekst, "[0-9]+", ""), # Fjerner tall fra teksten
tekst = str_squish(tekst), # Fjerner whitespace
tekst = str_replace_all(tekst, "\\b\\w{1,1}\\b", ""), # Fjerner enkeltbokstaver
tekst = str_replace_all(tekst, stoppord_boundary, ""), # Fjerner stoppord
tekst = str_replace_all(tekst, "[:punct:]", "")) # Fjerner all punktsetting
View(no4_prepped)
no4_tekster <- tempfile() # Oppretter en midlertidig fil på PCen
writeLines(text = no4_prepped %>% pull(tekst), con = no4_tekster) # I denne filen skriver vi inn teksten fra datasettet.
library(fastTextR)
ft_cbow <- ft_train(no4_tekster,
type = "cbow", # Velger cbow modell
control = ft_control(window_size = 5L)) # Setter kontekstvinduet til 5
View(ft_cbow)
ft_skipgram <- ft_train(no4_tekster,
type = "skipgram", # Velger skipgram modell
control = ft_control(window_size = 5L))
ft_word_vectors(ft_cbow, c("fordi", "himmel"))
ft_nearest_neighbors(ft_cbow, "himmel", k = 5L)
View(no4_tokens)
View(no4)
